# Starting to work with Pandas

# Background to this example

When working in Python for data analysis, the Pandas packages is one most commonly used packages. Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. It allows you to load, manipulate, analyse, visualize and write data in other file formats and ways than the original data.

**In this practical you will learn about**

-   **Explore data**

-   **Manipulating data**

-   **Dealing with missing values**

-   **Simple visuals**

-   **Working with categorical data**

# Getting started

**In this practical we will start exploring some of the basic functions in Pandas, learn about the different formats and ways you can use Pandas.**

There are lots of resources to help you with using Pandas and provide nice tips, trick and examples. For example you can use a **cheat sheet** to quickly remember and double check which functions to use (e.g. <https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf>) or useful pages that describe the [most basic functions of Pandas](https://pandas.pydata.org/docs/user_guide/10min.html). You can also find lots of good examples that use Pandas online for different types of data and types of analyses (<https://realpython.com/search?q=pandas>).

Let's start with using Python again by opening your Conda environment and then opening Spyder (for detailed instructions please look back at the instruction guide). We start by loading some of the standard libraries in this course. We use:

-   Pandas (data management and data handling)

-   Numpy (statistical analysis and data handling)

-   Matplotlib (plotting)

-   Scipy (statistical analysis)

## Making your first dataframe

``` python
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
```

Let us first generate some random numbers to work with, for that we sample from normal distribution with mean ($\mu$) = 0 and a standard deviation ($\sigma$) of 1. For this we use the Numpy package and more specifically the function [random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html). We will create two random datasets to play around with and explore the potential of Pandas. For the purpose of this tutorial we also set the [seed](https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html) of the random number generator. A random seed (or seed state, or just seed) is a number (or vector) used to initialize a pseudorandom number generator. This ensure that when we select the random numbers in for example a exercise like this we always draw the same "random" numbers, and results are comparable between students.

``` python
np.random.seed(1)
randomData_1 = np.random.normal(loc=0, scale=1, size=100)
randomData_2 = np.random.normal(loc=0, scale=1, size=100)
```

Then we put the the random data into a [Pandas Dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for easier manipulation. For this we merge the two datasets together, labeling the first one as 'A' and the second dataset as 'B'.

``` python
df = pd.DataFrame({'A':randomData_1, 'B':randomData_2}, columns=['A','B'])
```

``` python
           A         B
0   1.624345 -0.447129
1  -0.611756  1.224508
2  -0.528172  0.403492
3  -1.072969  0.593579
4   0.865408 -1.094912
..       ...       ...
95  0.077340 -1.627438
96 -0.343854  0.602319
97  0.043597  0.420282
98 -0.620001  0.810952
99  0.698032  1.044442

[100 rows x 2 columns]
```

When you look at the Dataframe you will see a couple of typical Pandas things, firstly you see the randomly generated numbers, you see the column labels 'A' and 'B' and lastly you see the numbers 0 to 99 indicating the rows numbers or in this case also the index of the dataframe. Both the [index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.index.html) and the [columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html) are very important elements as they provide unique identifiers for each individual data entry in the dataframe, a bit like coordinates on a map. Also note that Python (and thus Pandas) by defaults start counting at 0 rather than 1 which is sometimes the case for other programming languages.

The index and column names can be changed with the [rename](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) and [set_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html) function. Use the function pages you can try and follow the examples below

``` python
df = df.rename(columns={"B":"C"})
```

``` python
           A         C
0   1.624345 -0.447129
1  -0.611756  1.224508
2  -0.528172  0.403492
3  -1.072969  0.593579
4   0.865408 -1.094912
..       ...       ...
95  0.077340 -1.627438
96 -0.343854  0.602319
97  0.043597  0.420282
98 -0.620001  0.810952
99  0.698032  1.044442

[100 rows x 2 columns]
```

Or changing the index using

``` python
df = df.set_index(np.arange(10,110))
```

``` python
            A         C
10   1.624345 -0.447129
11  -0.611756  1.224508
12  -0.528172  0.403492
13  -1.072969  0.593579
14   0.865408 -1.094912
..        ...       ...
105  0.077340 -1.627438
106 -0.343854  0.602319
107  0.043597  0.420282
108 -0.620001  0.810952
109  0.698032  1.044442

[100 rows x 2 columns]
```

You will see now that the index have changed, ranging from 10 to 109 rather than 0 to 99 before. One of the powerful characteristics of Pandas is that it can also use dates as an index and that you can use these to do some time operations. To make a string of correctly formatted dates, you can use the [date_range](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html) function. The function requires either a start and end date or a length and frequency of the date format. If you look at the example below you see that we use the len function to find the total number of rows and we use the freq = 'D' which gives daily values. You can try changing it to other frequencies as well, like for example 'W' for weekly or 'Y' for yearly (more options can be found [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases)). We give an example below

``` python
df = df.set_index(pd.date_range(start='1/1/2020', periods=len(df), freq='D'))
```

``` python
                   A         C
2020-01-01  1.624345 -0.447129
2020-01-02 -0.611756  1.224508
2020-01-03 -0.528172  0.403492
2020-01-04 -1.072969  0.593579
2020-01-05  0.865408 -1.094912
             ...       ...
2020-04-05  0.077340 -1.627438
2020-04-06 -0.343854  0.602319
2020-04-07  0.043597  0.420282
2020-04-08 -0.620001  0.810952
2020-04-09  0.698032  1.044442

[100 rows x 2 columns]
```

Now we can do all kind of things to explore or manipulate the data. Firstly we can look at some common statistical properties, like the [mean](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html), [min](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.min.html), [max](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.max.html) or [median](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.median.html) values of rows and columns.

``` python
df.mean()
```

This will give you the mean over the columns. As you can see in the documentation of Pandas [mean](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html) you can also specify the axis which is set by default to 0, which means that it will take the mean over the different rows/indexes. This results in two numbers, one is the mean for column A and the other for column C. If you change this axis setting to 1 you will get the mean over the columns and thus a mean per row, resulting in 100 values in this case. You can also set it to `axis=None` which will give you the mean over the entire dataframe

``` python
df.mean(axis=None)
```

``` python
0.1066888148479486
```

#### Question 1

*Now try and compute the min, max and median for the same dataset.*

We can also do other forms of data manipulation, for example with the [resample](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html) function that can help you to aggregate or resample data to different frequencies. This is for example very useful when it comes to calculating long term average, or going from hourly data to daily data. For the sake of the example we will continue to keep working with the same dataframe, but of course in the rest of you professional life you will more likely do this for real data.

``` python
df.resample("W").mean()
```

``` python
                   A         C
2020-01-05  0.055371  0.135907
2020-01-12 -0.263757 -0.190761
2020-01-19 -0.240095 -0.215449
2020-01-26  0.321162  0.318309
2020-02-02 -0.367397 -0.055023
2020-02-09 -0.001452  0.134925
2020-02-16 -0.075634  0.296567
2020-02-23  0.184811  0.287754
2020-03-01  0.366306 -0.219428
2020-03-08  0.647443  0.067930
2020-03-15  0.027477  0.410324
2020-03-22 -0.133059  0.031299
2020-03-29  0.106092  0.307538
2020-04-05  0.285768  0.500578
2020-04-12 -0.055556  0.719499
```

Here you have the weekly values, please not that for this instance Python will use the end of the working week as cutoff.

#### Question 2

*Use the [resample](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html) documentation to make 2-daily average of your original dataframe. You can use the example in the documentation as a source of inspiration.*

# Exploring and manipulating the data

When you have your data in the system it will be useful to explore the properties of your data to decide how you will continue with further analysis. For this you can use the [describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) function which gives you some basic properties of your data.

``` python
df.describe()
```

``` python
                A           C
count  100.000000  100.000000
mean     0.060583    0.152795
std      0.889615    0.936690
min     -2.301539   -2.434838
25%     -0.613818   -0.300010
50%      0.064074    0.236616
75%      0.637410    0.743020
max      2.185575    2.528326
```

It can also be convenient to order your data in a logical array or order, for this you can use the [sort_index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_index.html) or [sort_values](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) functions.

``` python
df.sort_index(axis=1, ascending=False)
```

``` python
df.sort_values(by="A", ascending=False)
```

#### Question 3

*What is the difference between sort_index or sort_values? When is it useful to sort on the index and when to sort on the values, can you give some examples?*

If you just want to look at one column or get the data from one column you can used the code below

``` python
df["A"]
df["A"].values
```

#### Question 4

*What is the difference between the two outcomes? Look at the data type with the [type](https://docs.python.org/3/library/functions.html#type) function? What is the downside of df\["A"\].values?*

Maybe you are interested in selecting a single row from the dataframe, you can do this using the [loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html) function

``` python
df.loc["2020-01-01"]
```

Or maybe combine it with a specific column as well

``` python
df["A"].loc["2020-01-01"]
```

You can also use the [iloc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) function

``` python
df.iloc[0:3]
```

Or again for the specific column

``` python
df["A"].iloc[0:3]
```

#### Question 5

*What is the difference between the loc and iloc functions in terms of practical use?*

You can also do conditional selection of data using the example below. All value that do not satisfy the condition will be transformed into NaN (Not a Number).

``` python
df[df > 0]
```

Or if you want that condition to apply to a specific column you also define the column. Please note that you still get the output for both columns, but it is conditioned on just column "A"

``` python
df[df["A"] > 0]
```

Maybe you want to correct a specific value of a dataframe. For this you can use the [at](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html) function.

``` python
df.at["2020-01-01", "A"] = 0
```

Or the [iat](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iat.html) function which is similar to iloc but works with integers, just like a normal matrix selection in Python.

``` python
df.iat[0, 1] = 0
```

#### Question 6

*Describe in your own words what the function below do in terms of data manipulation or selection*

``` python
df1 = df[df > -1.0]
```

``` python
df1.describe()
```

#### Question 7

*What do you notice compared to df.describe()?*

# Dealing with missing values

Sometimes there are missing values in a timeseries. You can deal with this in a variety of ways, either removing the data or filling it with new values. We have several options for handling missing data in Python

We can drop all rows where missing data exists using the [dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html) function.

``` python
df1.dropna(how="any")
```

Or we can fill up the missing values with a standard value using the fillna function

``` python
df1.fillna(value=5)
```

We can also interpolate between the missing values. Please note this only makes sense if there is a logical order to the datapoints, for example if it is a timeseries.

``` python
df1.interpolate()
```

We can also select the rows that contain missing values if we need them.

``` python
df1.isna()
```

Please remember these options as they might come in handy later when you are dealing with data that doesn't look nice or pretty and contains missing values.

# Merging different data

Sometimes we have one dataset and we want to add new data to it. You can think of data coming in different files and it needs to be merged. We now first create a new dataset which only contains 5 datapoints and join it with the "old" dataset (df) that already had 100 datapoints. For this we use the [join](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html) function. When doing this always make sure that the indexes of the different dataframes have a similar range or values.

``` python
randomData_3 = np.random.normal(loc=0, scale=1, size=5)
df2 = pd.DataFrame({'D':randomData_3}, columns=['D'], index=pd.date_range(start='1/1/2020', periods=len(randomData_3), freq='D'))
df3 = df.join(df2)
```

#### Question 8

*Describe what you see in the description of the dataframe. Anything that jumps out to you?*

``` python
df3.describe()
```

# Simple visualizations of your data

We can also make simple [plot](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)s of the dataframe. Pandas will use some default plotting options, which by default is a lineplot. Please not that by default NaN data will be excluded from the plot.

``` python
df3.plot()
```

We can also combine operations before plotting the data using [cumsum](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html) (cumulative sum) and then plot the data from there.

``` python
df3.cumsum().plot()
```

We can also make different kinds of plots, like this scatterplot for example.

``` python
df3.plot(kind='scatter', x='A',y='C')
```

#### Question 9

*Explore more options in the plot function, specifically altering the kind. Provide at least two relevant examples.*

# Working with categorical data

``` python
weekDays = pd.date_range(start='1/1/2020', periods=100, freq='D').strftime("%A")
df4 = pd.DataFrame({'Weekday':pd.Categorical(weekDays)}, columns=['Weekday'], index=pd.date_range(start='1/1/2020', periods=len(weekDays), freq='D'))
df5 = df3.join(df4)
```

We have now added a column with all the weekday. In the first line we transformed the datetime array to week names using the comment `.strftime("%A")`. You can use strftime for all kind of time conversation or rewriting options. You can explore <https://strftime.org> for options, they are also used in other programming languages. The `pd.Categorical` is used to enforce that Pandas sees the data as categorical. You can check this by using

``` python
df5.dtypes

A           float64
C           float64
D           float64
Weekday    category
dtype: object
```

``` python
df5.groupby("Weekday").mean()
```

#### Question 10

*Describe in your own words what you see after running the groupby("Weekday") command.*

# What you have learned today

**If all is well you have learned today:**

-   **Explore data**

-   **Manipulating data**

-   **Dealing with missing values**

-   **Simple visuals**

-   **Working with categorical data**
