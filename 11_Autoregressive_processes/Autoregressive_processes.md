# Autoregressive processes
Emilia Jarochowska

Before we analyze some real-life datasets, let’s first fabricate some.
Why fabricate data? This is the principle of simulations, which we often
use to e.g. test the performance of a method, or to understand
*mechanically* how a process works. So to check if a test or a
regression model detects the right thing, we might want to simulate a
process with known parameters and check how well it is recovered by the
regression model we fit.

A second motivation is to give you an intuition as to what common types
of processes are. We will look at the simplest stochastic process,
random walk, and a simple autoregressive process, also known as red
noise. Their manifestations can look deceivingly similar in a time
series plot, but they have very different properties. Both are widely
used in geosciences.

# What causes autocorrelation?

To understand what an autoregressive process is, let’s first consider a
simple example. Suppose we have a time series $X_t$ that is generated by
the following equation:

``` python
import random
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_process import ArmaProcess
from scipy import stats
```

## Task 1

Can you write your own code for a random walk process in one dimension?
Write an algorithm which generates a vector of values, where each
subsequent value increases by 1 or decreases by 1 with equal
probabilities. There are many ways of writing this algorithm and all of
them are useful to understand what you are doing, so give it a try
before running the solution. Maybe you can come up with a better one?

<details>
<summary>
Solution
</summary>

``` python
def randomwalk1D(n):
    x, y = 0, 0
    # Generate the time points [1, 2, 3, ... , n]
    timepoints = np.arange(n + 1)
    positions = [y]
    directions = ["UP", "DOWN"]
    for i in range(1, n + 1):
        # Randomly select either UP or DOWN
        step = random.choice(directions)
        
        # Move up or down
        if step == "UP":
            y += 1
        elif step == "DOWN":
            y -= 1
        # Keep track of the positions
        positions.append(y)
    return timepoints, positions

time_data, pos_data = randomwalk1D(1000)
plt.plot(time_data, pos_data, 'r-')
plt.title("1D Random Walk")
plt.show()
```

</details>

The code in the solution is useful to understand how the random walk is
generated. But there is another handy way of simulating a random walk,
which makes use of the fact that the increases and decreases in an
unbiased random walk are normally distributed:

``` python
np.random.seed(123)
y = np.cumsum(np.random.normal(loc=0, scale=1, size=1000))
plt.plot(y[1:500])
```

This of course looks like a strong trend. If this was diversity, we
would say “mass extinction”. If this was temperature, we would be back
to Snowball Earth. Is this really a random walk? Let’s plot the rest of
it.

``` python
plt.plot(y[500:1000])
```

It even has regular-looking zig-zaggy peaks that make it look as if
there was some periodicity in this process. This gives you an idea how
easy it is to see patterns in random wiggles. That’s why we need
statistical models to tell if a process is anything different from
random noise. So how can we characterize it?

## Autocorrelation function

``` python
plot_acf(y)
```

This plot shows you that a random walk is strongly autocorrelated. We
can extract the values of autocorrelation at different lags using the
`acf` function:

``` python
acf(y)
```

# Autoregressive processes

An autoregressive process is usually just part of a more complex
empirical time series. But we need to understand and isolate it to
decompose an empirical time series into its components.

The simplest autoregressive process is of order 1, also written AR(1),
and can be modeled as $X_t = \beta + \alpha X_{t-1} + \epsilon_t$.

This is also known as the “red noise” and is very important in many
modeling applications. For example, you will encounter it in
cyclostratigraphy (as the null hypothesis in testing for astronomical
forcing) or [in paleobiology when studying trait
evolution](https://stratigraphicpaleobiology.shinyapps.io/DarwinCAT/) in
the absence of selection (aka “genetic drift”).

We can generate autoregressive processes with various parameters using
the `tsa` module in `statsmodels`. The function `ArmaProcess` stands for
“AutoRegressive Moving Average”, but now we are only interested in the
autoregressive part so we ignore the Moving Average part. We only set
the zero-lag coefficient of 1. The parameters of an AR process will be 1
(order) and $\alpha$. Because of conventions in the time series
literature in signal processing, the sign of $\alpha$ has to be flipped
. For example, for an AR(1) process with $\alpha = 0.9$, the array
representing the AR parameters would be `ar = np.array([1, -0.9])`.

``` python
ar1 = np.array([1, -0.9])
ma1 = np.array([1])
AR_object = ArmaProcess(ar1, ma1)
simulated_data = AR_object.generate_sample(nsample=1000)
plt.plot(simulated_data);
```

## Task 2

Just to be sure you understand what the `acf` function is doing, can you
try to construct processes with the following properties?

1.  With the strongest correlation at lag 2? Remember that this is
    described by AR-coefficients $\alpha_k$, for *k = 1, 2,…, p*
2.  With negative autocorrelation (also called a “repulsive” process)

Make an `acf` plot for each of the processes to check if you solved it
correctly. Many solutions are possible.

# Testing for autocorrelation

Because time series can be autocorrelated in different ways, it is not
easy to test for it. The simplest way is to use the Durbin-Watson test.
This test is based on the residuals of the regression of $X_t$ on
$X_{t-1}$. We will use it once we fit a model. But now we don’t have a
model, we want to directly test if our dummy data `y` is autocorrelated,
even though we can already see it in the `acf` plot. In the absence of a
model, we can use a more general test, the Jarque-Bera goodness of fit
test for normality.

``` python
res = stats.jarque_bera(y)
res
```

What is this odd p-value? Please see `scipy`
[documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.jarque_bera.html)
to find out and check when this test can be used. Are we using it
correctly here?

# Partial autocorrelation function

Partial correlation, in general, is the correlation between two
variables after removing the effect of other variable(s). In time
series, this is the correlation between $X_t$ and $X_{t-k}$ after
removing the effect of $X_{t-1}, X_{t-2}, ..., X_{t-k+1}$. In practice,
we use the partial autocorrelation function to identify the order of an
autoregressive process. You will see `p` sticking out as a significant
lag in the PACF:

What would you detect as the order of the random walk you generated?

``` python
plot_pacf(y)
```

And what is the order of the AR(1) process we generated?

``` python
plot_pacf(simulated_data)
```

Just to be sure it works, let’s also make a process of order 2:

``` python
ar3 = np.array([1, -0.6, -0.3])
ma3 = np.array([1])
AR_object3 = ArmaProcess(ar3, ma3)
simulated_data3 = AR_object3.generate_sample(nsample=1000)
plt.plot(simulated_data3);
```

``` python
plot_pacf(simulated_data3)
```

Can you read out the order of the process from the PACF plot?
